{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64779219-57f7-4452-b24f-b2a246b92c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "import random\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "training_dataset = load_dataset('stanfordnlp/imdb', split='train')\n",
    "testing_dataset = load_dataset('stanfordnlp/imdb', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "512620c6-d7e3-49c0-b70e-fd939be9297f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/bchk/eibarra1/hw5/wandb/run-20251101_152134-qz82lqya</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ibarra000-northeastern-university/huggingface/runs/qz82lqya' target=\"_blank\">helpful-durian-6</a></strong> to <a href='https://wandb.ai/ibarra000-northeastern-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ibarra000-northeastern-university/huggingface' target=\"_blank\">https://wandb.ai/ibarra000-northeastern-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ibarra000-northeastern-university/huggingface/runs/qz82lqya' target=\"_blank\">https://wandb.ai/ibarra000-northeastern-university/huggingface/runs/qz82lqya</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 01:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>3.143100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>3.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3030</td>\n",
       "      <td>3.153500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>3.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>3.193200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>3.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3070</td>\n",
       "      <td>3.127300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>3.194000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>3.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3110</td>\n",
       "      <td>3.160400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>3.160400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "Saving model to ./fine-tuned-gpt2-large...\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "WANDB_NOTEBOOK_NAME = 'hw5/Direct-Preference-Optimization'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('openai-community/gpt2-large')\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./my_training_output\",\n",
    "    save_strategy = 'steps',\n",
    "    save_steps = 500,\n",
    "    report_to='wandb',\n",
    "    project=WANDB_NOTEBOOK_NAME,\n",
    "    num_train_epochs=1.0,\n",
    ")\n",
    "\n",
    "# Default Learning Rate of 2e-05\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    ")\n",
    "print(\"Starting training...\")\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "local_save_path = './fine-tuned-gpt2-large'\n",
    "print(f\"Saving model to {local_save_path}...\")\n",
    "trainer.save_model(local_save_path)\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d3ce67-520e-4e0b-b689-74d8ff05b608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13eaa2229454daf87bc26ed4cd97b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1ba3032b3d47bebc2a44bd3ee4fa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975dd88460ed48e4b68322d53fd5d5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4032d7b4c4e841e5b041a9bbc082e1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9d0f40014244f5b9827f4cee61b603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18d9862081747c8a2ba20a69cda3504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7bf70d961b43b9bfbd1eeffe45aaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded.\n"
     ]
    }
   ],
   "source": [
    "our_model = AutoModelForCausalLM.from_pretrained('./fine-tuned-gpt2-large')\n",
    "our_tokenizer = AutoTokenizer.from_pretrained('./fine-tuned-gpt2-large')\n",
    "\n",
    "\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"siebert/sentiment-roberta-large-english\")\n",
    "\n",
    "print(\"Models loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378779be-5dd3-4964-8eff-33a301c82dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Tokenizing prompts...\n"
     ]
    }
   ],
   "source": [
    "NUM_PROMPTS = 1000\n",
    "NUM_SAMPLES_PER_PROMPT = 4\n",
    "OUTPUT_FILE = \"generations.json\"\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.2,\n",
    "}\n",
    "# ---------------------\n",
    "\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "prefix_dataset = load_dataset('stanfordnlp/imdb', split='train')\n",
    "\n",
    "prompt_data = []\n",
    "print(\"Tokenizing prompts...\")\n",
    "for index in range(NUM_PROMPTS): \n",
    "    data = prefix_dataset[index]\n",
    "    prefix_length = random.randint(2, 8)\n",
    "    prompt_text = \" \".join(data['text'].split()[:prefix_length])\n",
    "    \n",
    "    tokens = our_tokenizer(prompt_text, return_tensors=\"pt\")\n",
    "    \n",
    "    prompt_data.append({\n",
    "        \"prompt_text\": prompt_text,\n",
    "        \"tokenized_inputs\": tokens,\n",
    "        \"original_label\": data['label']\n",
    "    })\n",
    "\n",
    "generation_params = generate_kwargs.copy()\n",
    "generation_params['num_return_sequences'] = NUM_SAMPLES_PER_PROMPT\n",
    "\n",
    "if \"pad_token_id\" not in generation_params:\n",
    "    generation_params['pad_token_id'] = our_tokenizer.eos_token_id\n",
    "\n",
    "json_output_data = []\n",
    "\n",
    "try:\n",
    "    print(\"Generating samples...\")\n",
    "    for data in tqdm(prompt_data):\n",
    "        inputs = data[\"tokenized_inputs\"]\n",
    "        prompt_text = data[\"prompt_text\"]\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Assumes our_model is loaded\n",
    "            generated_sequences = our_model.generate(\n",
    "                **inputs,\n",
    "                **generation_params\n",
    "            )\n",
    "            \n",
    "            decoded_samples = []\n",
    "            input_length = inputs[\"input_ids\"].shape[1]\n",
    "            \n",
    "            for seq in generated_sequences:\n",
    "                generated_tokens_only = seq[input_length:]\n",
    "                \n",
    "                # Assumes our_tokenizer is loaded\n",
    "                decoded_text = our_tokenizer.decode(\n",
    "                    generated_tokens_only,\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                decoded_samples.append(decoded_text.strip())\n",
    "            \n",
    "            json_output_data.append({\n",
    "                \"prompt\": prompt_text,\n",
    "                \"generations\": decoded_samples,\n",
    "            })\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating for prompt: '{prompt_text}'. Error: {e}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n--- Generation interrupted by user ---\")\n",
    "\n",
    "finally:\n",
    "    print(\"\\n--- Generation Complete or Interrupted ---\")\n",
    "    print(f\"Total prompts processed: {len(json_output_data)}\")\n",
    "    \n",
    "    if not json_output_data:\n",
    "        print(\"No results to save.\")\n",
    "    else:\n",
    "        print(f\"Saving {len(json_output_data)} results to {OUTPUT_FILE}...\")\n",
    "        try:\n",
    "            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "                json.dump(json_output_data, f, indent=4)\n",
    "            print(\"Successfully saved to JSON.\")\n",
    "            \n",
    "            if json_output_data:\n",
    "                print(\"\\nExample of first item saved:\")\n",
    "                print(json.dumps(json_output_data[0], indent=2))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to JSON file: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150032d-b18e-4599-8874-03e19fe2ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_tokenizer\n",
    "\n",
    "generation_data_for_sentiment\n",
    "\n",
    "scores = []\n",
    "\n",
    "for completion in generation_data_for_sentiment:\n",
    "    inputs = sentiment_tokenizer(completion, return_tensors=\"pt\")\n",
    "    score = sentiment_model.generate(**inputs, **generation_params)\n",
    "    scores.append(score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shared-bchk-venv",
   "language": "python",
   "name": "shared-bchk-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
